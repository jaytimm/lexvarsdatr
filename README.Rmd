
---
output:
  md_document:
    variant: markdown_github
---

```{r, echo = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "##",
  fig.path = "README-")
```



#lexvarsdatr

A collection of psycholinguistic/behavioral data, collated from supplemental materials and public databases. 
MORE than this.  And more general -- most people do not know what this means.

Perhaps highlight as a simple (and developing) cache of odds/ends resources.   

Some tools for investigating lexical variation --- both behavioral and distributional.

Data included in package:  Perhaps this needs to be pushed down.  More than a data package at this point.

```{r echo=FALSE, message=FALSE, warning=FALSE}
library(knitr)
Data <- c("Lexical decision and naming","Concreteness ratings","AoA ratings", "Word association")
```


```{r echo=FALSE, message=FALSE, warning=FALSE}
Source <- c( "Balota, D. A., Yap, M. J., Hutchison, K. A., Cortese, M. J., Kessler, B., Loftis, B., ... & Treiman, R. (2007). The English lexicon project. *Behavior research methods*, 39(3), 445-459.","Brysbaert, M., Warriner, A. B., & Kuperman, V. (2014). Concreteness ratings for 40 thousand generally known English word lemmas. *Behavior research methods*, 46(3), 904-911.","Kuperman, V., Stadthagen-Gonzalez, H., & Brysbaert, M. (2012). Age-of-acquisition ratings for 30,000 English words. *Behavior Research Methods*, 44(4), 978-990.","Nelson, D. L., McEvoy, C. L., & Schreiber, T. A. (2004). The University of South Florida free association, rhyme, and word fragment norms. *Behavior Research Methods, Instruments, & Computers*, 36(3), 402-407.")

kable(data.frame(Data=Data,
                 Source=Source))
```


##Installation

```{r message=FALSE, warning=FALSE, paged.print=FALSE}
library(devtools)
#devtools::install_github("jaytimm/lexvarsdatr")
library(lexvarsdatr) 
```



##Usage

### Behavioral data

For convenience, response times in lexical decision/naming, concreteness ratings, and AoA ratings have been collated into a single data frame, `lex_behav_data`.  Approximately 18K word forms are included in all three data sets.  

```{r message=FALSE, warning=FALSE}
library(tidyverse)
lexvarsdatr::lvdr_behav_data %>%
  na.omit %>%
  head
```



### Word association data

The South Florida word association data set lives in `lvdr_association`.  A description of variables included in the normed data set, as well as methodologies, can be found [here](http://w3.usf.edu/FreeAssociation/).  

The `lvdr_get_associates` function enables quick access to word associates for a given cue; associates are listed in descending order (per total subject responses).

Perhaps rename -- or eliminate -- ?

```{r}
lexvarsdatr::lvdr_get_associates(cue='think')
```

Somehow -- comparing word association data & co-occurrence data.  Relates most to applying both functions across full lexicons.



FUNCTIONS >> 

### Build PPMI Matrix

A simple function for transforming a count-based co-occurrence matrix.  

We could use some publically available - generic text to demonstrate.  

```{r}
install.packages('sotu')
library(sotu)

sotu_eg <- sotu::sotu_text

## Perhaps we can mess around with phrases/terms here. ?? 
```

A few functions that fill in some workflow gaps in VSM/word embedding-based models of distributional semantics.

A `get_collocates` function, within this framework -- ?   From a term-feature matrix -- as `dgCMatrix`.


Recall: `udpipe::as_cooccurrence()`: which converts tcm to term1-term2-freq dataframe.  -- Perhaps just easiest.  Because what we want to do here is functionally equivalent (with the exception of finding a single term's collocates.)

Recall: word association sparse is not ppmi transformed. 

As distinct from `get_neighbors` -- which we could do as well.  (As we have written two functions up this point.) -- SVD and text2vec output are the same I think.  Function would not be ideal for maths/analogy, etc.

`get_neighbors` would work on some type of count-based matrix.  Presumably reduced in some way, via SVD etc.  Simple cosine similarity.  (Cosine with word2vec/neural net -- ?).

And potentially: a `get_all_neighbors` function.  At present, this is impressively slow.  

```{r message=FALSE, warning=FALSE}
t2v_ents = text2vec::itoken(sotu::sotu_text, 
                            preprocessor = tolower, 
                            tokenizer = text2vec::word_tokenizer, 
                            ids = 1:236)

vocab = text2vec::create_vocabulary(t2v_ents, stopwords = tm::stopwords()) 

pruned_vocab = text2vec::prune_vocabulary(
  vocab, term_count_min = 10, doc_proportion_max = 0.75) #, doc_proportion_max = 0.95

tcm = text2vec::create_tcm(t2v_ents, 
                           vectorizer = text2vec::vocab_vectorizer(pruned_vocab), 
                           skip_grams_window = 5L)

tcm <- as(tcm, 'dgCMatrix')
tcm <- tcm %>% lexvarsdatr::lvdr_build_sparse_ppmi()
```



```{r}
network <- lvdr_build_network(tfm = tcm, 
                              #target = c('china', 'france', 'germany'), 
                              #target = c('behind', 'belief', 'discover'),
                              target = c('enemy', 'ally', 'friend', 'partner'),
                              n = 10)

network <- lvdr_build_network(tfm = lexvarsdatr::lvdr_association_sparse, 
                              target = toupper(c('behind', 'belief', 'discover')), 
                              n = 75)
```

Understand present network building function.  NODES: search terms + 1st order collocates.

EDGES: Connections among nodes.  NOTE: In a weighted matrix, top/bottom won't be symmetrical.


TIF text -> term-feature matrix (co-occurrence) -> ppmi-transformed matrix.  

How would `get_neighbors` -> `build_network` work -- ?  Clearly, not unrelated workflows.  If distinct, former would seem most relevant for a single form.

```{r}
lvdr_get_syns <- function(tfm, target) {
  
  
  
  
}
```


### Build network structure

Demonstrate as generic.  Works on term-feature matrix of some variety.  


The `lvdr_build_network` function builds a network structure for a given search term.  We need to describe some details here RE color/shape and ppmi strength parameters.  Friends of Friends, as well.  

```{r}
#network <- lexvarsdatr::lvdr_build_network("hindsight") 

network <- lexvarsdatr::lvdr_association_sparse %>%
  lexvarsdatr::lvdr_extract_network(search = c('EAST', 'WEST'), 
                                    tf_min = 1, ff_min = 1) ##These parameters need to be re-thought.
```


Output object contains nodes/edges dataframes, and can be fed directly to any number of network visualization R packages.

NOTE: error only surfaces in viz process below.  7/21/19.


```{r fig.height=7, message=FALSE, warning=FALSE}
set.seed(66)
network %>%
  tidygraph::as_tbl_graph() %>%
  ggraph::ggraph() +
  
  ggraph::geom_edge_link(color = 'darkgray') + #alpha = 0.8
  ggraph::geom_node_point(aes(size = value, 
                              color = term,
                              shape = group)) +
  
  ggraph::geom_node_text(aes(label = toupper(label), 
                             filter = group == 'term'), 
                             repel = TRUE, size = 4) +
  
  ggraph::geom_node_text(aes(label = tolower(label), 
                             filter = group == 'feature'), 
                             repel = TRUE, size = 3) +
  
  theme_minimal() +
  theme(legend.position = "none",
        plot.title = element_text(size=11),
        axis.text.x = element_blank(),
        axis.text.y = element_blank())
```


```{r}
network %>%
    tidygraph::as_tbl_graph() %>%
      ggraph::ggraph() +
      ggraph::geom_edge_link(color = 'darkgray') + #alpha = 0.8
      ggraph::geom_node_point(aes(size = value, 
                                  color = search_term,
                                  shape =group)) +
      ggraph::geom_node_text(aes(label = label, 
                                 filter = group == 'term'), 
                             repel = TRUE, size = 2.35)+
      ggthemes::scale_color_stata()+
      ggthemes::theme_fivethirtyeight() +
      ggtitle(names(net1[i]))+
      theme(legend.position = "none",
            plot.title = element_text(size=11),
            axis.text.x = element_blank(),
            axis.text.y = element_blank())
```



